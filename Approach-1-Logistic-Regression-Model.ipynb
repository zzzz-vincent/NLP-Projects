{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Logistic Regression Model\n",
    "\n",
    "This approach builds a Logistic Regression model to predict the safety of product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade dependencies\n",
    "! which python3\n",
    "! pip3 install -r ../data/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from os import path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "\n",
    "This approach uses the __pandas__ library to read our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/training.csv', encoding='utf-8', header=0)\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Test data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/test.csv', encoding='utf-8', header=0)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Logistic Regression Model\n",
    "Apply pre-processing and vectorization operations and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values for each columm\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill-in the missing values in it with the empty string\n",
    "train_df[\"text\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of stop words from the NLTK library\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "# Excluding useful words\n",
    "excluding = [\n",
    "    \"against\",\n",
    "    \"not\",\n",
    "    \"don\",\n",
    "    \"don't\",\n",
    "    \"ain\",\n",
    "    \"aren\",\n",
    "    \"aren't\",\n",
    "    \"couldn\",\n",
    "    \"couldn't\",\n",
    "    \"didn\",\n",
    "    \"didn't\",\n",
    "    \"doesn\",\n",
    "    \"doesn't\",\n",
    "    \"hadn\",\n",
    "    \"hadn't\",\n",
    "    \"hasn\",\n",
    "    \"hasn't\",\n",
    "    \"haven\",\n",
    "    \"haven't\",\n",
    "    \"isn\",\n",
    "    \"isn't\",\n",
    "    \"mightn\",\n",
    "    \"mightn't\",\n",
    "    \"mustn\",\n",
    "    \"mustn't\",\n",
    "    \"needn\",\n",
    "    \"needn't\",\n",
    "    \"shouldn\",\n",
    "    \"shouldn't\",\n",
    "    \"wasn\",\n",
    "    \"wasn't\",\n",
    "    \"weren\",\n",
    "    \"weren't\",\n",
    "    \"won\",\n",
    "    \"won't\",\n",
    "    \"wouldn\",\n",
    "    \"wouldn't\",\n",
    "]\n",
    "\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "# Initialize the Stemmer\n",
    "snow = SnowballStemmer(\"english\")\n",
    "\n",
    "# Process the text for cleaning\n",
    "def process_text(texts):\n",
    "    final_text_list = []\n",
    "    for sent in texts:\n",
    "        \n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = \"\"\n",
    "            \n",
    "        filtered_sentence = []\n",
    "        \n",
    "        # String Clearning:\n",
    "        # Lowercase\n",
    "        sent = sent.lower()\n",
    "        # Remove leading/trailing whitespace\n",
    "        sent = sent.strip()\n",
    "        # Remove extra space and tabs\n",
    "        sent = re.sub(\"\\s+\", \" \", sent)\n",
    "        # Remove HTML tags/markups:\n",
    "        sent = re.compile(\"<.*?>\").sub(\"\", sent)\n",
    "\n",
    "        for w in word_tokenize(sent):\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if (not w.isnumeric()) and (len(w) > 2) and (w not in stop_words):\n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence)  # final string of cleaned words\n",
    "\n",
    "        final_text_list.append(final_string)\n",
    "\n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[[\"text\"]],\n",
    "    train_df[\"human_tag\"].values,\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Val data clean\n",
    "print(\"Processing the text fields...\")\n",
    "X_train[\"text\"] = process_text(X_train[\"text\"].tolist())\n",
    "X_val[\"text\"] = process_text(X_val[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TD-IDF to vectorize to vectors of len 750.\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features=750)\n",
    "\n",
    "# Fit the vectorizer to training data\n",
    "tf_idf_vectorizer.fit(X_train[\"text\"].values)\n",
    "\n",
    "# Transform text fields\n",
    "X_train = tf_idf_vectorizer.transform(X_train[\"text\"].values).toarray()\n",
    "X_val = tf_idf_vectorizer.transform(X_val[\"text\"].values).toarray()\n",
    "\n",
    "# Check data size\n",
    "print(\"Shapes of features: Training and Validation\")\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store vectorized datasets for further adjustment\n",
    "X_train_store = X_train.copy()\n",
    "X_val_store = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign stored data to X_train and X_val, used for second and later round model optimzation\n",
    "X_train = X_train_store.copy()\n",
    "X_val = X_val_store.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vectorization results\n",
    "tf_idf_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch numbers for each weight update\n",
    "batch_size = 16\n",
    "# Set epochs for total number of iterations\n",
    "epochs = 30\n",
    "# Set Learning rate\n",
    "lr = 0.005\n",
    "\n",
    "# Run the training in the GPU if supported, else in the CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build our double layers network\n",
    "net = nn.Sequential(\n",
    "    # Input size of 1 is expected. Linear layer-1 with 10 units\n",
    "    nn.Linear(in_features=750, out_features=10),\n",
    "    # Relu activation is applied\n",
    "    nn.ReLU(),\n",
    "    # Output layer with single unit\n",
    "    nn.Linear(10, 1),\n",
    "    # Add Sigmoid at the end to turn output to probabilities\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "net.to(device)\n",
    "\n",
    "# Initialize the network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_weights)\n",
    "\n",
    "# Define the loss. For binary classification the appropriate choice is Binary Cross Entropy.\n",
    "# For sigmoid in the last layer, use nn.BCELoss.\n",
    "loss = BCELoss(reduction=\"none\")\n",
    "\n",
    "# Define the optimizer, SGD (Stochastic Gradient Descent) with learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# Use PyTorch DataLoaders to load the data in batches\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32),\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# Move validation dataset on CPU/GPU device\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the losses as the training progresses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    # Build a training loop to train the network\n",
    "    for data, target in train_loader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device).view(-1, 1)\n",
    "\n",
    "        # Forward pass - compute the predictions of the NN on the batch\n",
    "        output = net(data)  \n",
    "        # Compute the loss and sum\n",
    "        L = loss(output, target).sum()\n",
    "        training_loss += L.item() \n",
    "        # Calculate gradients\n",
    "        L.backward()  \n",
    "        # Update weights with gradient descent\n",
    "        optimizer.step()  \n",
    "\n",
    "    # Get validation predictions\n",
    "    val_predictions = net(X_val)\n",
    "    # Calculate the validation loss\n",
    "    val_loss = torch.sum(loss(val_predictions, y_val.view(-1, 1))).item()\n",
    "\n",
    "    # Take the average losses\n",
    "    training_loss = training_loss / len(y_train)\n",
    "    val_loss = val_loss / len(y_val)\n",
    "\n",
    "    train_losses.append(training_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"Epoch {epoch}. Train_loss {training_loss}, Validation_loss {val_loss}, Seconds {end-start}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss values\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.ylim([0.25, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictions\n",
    "\n",
    "Make Predictions with test data\n",
    "1. Fill-in missing values\n",
    "2. Clean and normalize text\n",
    "3. Vectorization\n",
    "4. Convert to Torch tensor\n",
    "5. Get predictions\n",
    "6. Round up to 1 or down to 0\n",
    "\n",
    "You will save your predictions (with __test_predictions__ variable) to a CSV file later in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill-in missing values\n",
    "test_df[\"text\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and normalize text\n",
    "test_df[\"text\"] = process_text(test_df[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "X_test = tf_idf_vectorizer.transform(test_df[\"text\"].values).toarray()\n",
    "\n",
    "# Chech vectorization results\n",
    "print(\"Shapes of features: Training and Validation\")\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Torch tensor\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predictions\n",
    "test_predictions = net(X_test)\n",
    "#Round up to 1 or down to 0\n",
    "test_predictions = np.rint(test_predictions.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write predictions to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = test_df[\"ID\"]\n",
    "result_df[\"human_tag\"] = test_predictions\n",
    " \n",
    "result_df.to_csv(\"../../data/approach_1_result.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-env",
   "language": "python",
   "name": "tutorial-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
